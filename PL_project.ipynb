{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PL-project.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM39qMPRT7NWa7Ka6RbvPkd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paddy-la/LagrangianNN/blob/main/PL_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ947PLfkvGX",
        "outputId": "a5f5cc4d-8e31-4193-d562-6f8042937042"
      },
      "source": [
        "!pip install --upgrade https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.36-cp36-none-linux_x86_64.whl\n",
        "!pip install --upgrade jax  # install jax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jaxlib==0.1.36\n",
            "\u001b[?25l  Downloading https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.36-cp36-none-linux_x86_64.whl (48.2MB)\n",
            "\u001b[K     |████████████████████████████████| 48.2MB 66kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.36) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.36) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.36) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.36) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.36) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->jaxlib==0.1.36) (50.3.2)\n",
            "Installing collected packages: jaxlib\n",
            "  Found existing installation: jaxlib 0.1.57+cuda101\n",
            "    Uninstalling jaxlib-0.1.57+cuda101:\n",
            "      Successfully uninstalled jaxlib-0.1.57+cuda101\n",
            "Successfully installed jaxlib-0.1.36\n",
            "Requirement already up-to-date: jax in /usr/local/lib/python3.6/dist-packages (0.2.6)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bSxlpS8kxep",
        "outputId": "913b83b8-14aa-46db-c511-bc171cb69532"
      },
      "source": [
        "!pip install --upgrade -q jax==0.1.55 jaxlib==0.1.36\n",
        "!pip install -U -q Pillow moviepy proglog"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▎                              | 10kB 24.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20kB 14.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 30kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 40kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 51kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 61kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 81kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 92kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 102kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 112kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 122kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 133kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 143kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 153kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 163kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 174kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 184kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 194kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 204kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 215kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 225kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 235kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 245kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256kB 7.9MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fastcache (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 7.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 389kB 55.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 50.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 26.9MB 121kB/s \n",
            "\u001b[?25h  Building wheel for moviepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for proglog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1M7MORZknKw",
        "outputId": "b26d4ca5-76af-4e3e-c1f0-d28ddd6923c4"
      },
      "source": [
        "from jax.lib import xla_bridge\n",
        "print(xla_bridge.get_backend().platform)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzekK8NhIXxQ"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental.ode import odeint\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def lagrangian(q, q_dot, m1, m2, l1, l2, g):\n",
        "   \n",
        "    t1, t2 = q     # theta 1 and theta 2\n",
        "    w1, w2 = q_dot # omega 1 and omega 2\n",
        "\n",
        "    # kinetic energy (T)\n",
        "    T1 = 0.5 * m1 * (l1 * w1)**2\n",
        "    T2 = 0.5 * m2 * ((l1 * w1)**2 + (l2 * w2)**2 +\n",
        "                    2 * l1 * l2 * w1 * w2 * jnp.cos(t1 - t2))\n",
        "    T = T1 + T2\n",
        "  \n",
        "    # potential energy (V)\n",
        "    y1 = -l1 * jnp.cos(t1)\n",
        "    y2 = y1 - l2 * jnp.cos(t2)\n",
        "    V = m1 * g * y1 + m2 * g * y2\n",
        "\n",
        "    return T - V\n",
        "\n",
        "\n",
        "def f_analytical(state, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "    t1, t2, w1, w2 = state\n",
        "\n",
        "    a1 = (l2 / l1) * (m2 / (m1 + m2)) * jnp.cos(t1 - t2)\n",
        "    a2 = (l1 / l2) * jnp.cos(t1 - t2)\n",
        "\n",
        "    f1 = -(l2 / l1) * (m2 / (m1 + m2)) * (w2 ** 2) * jnp.sin(t1 - t2) - (g / l1) * jnp.sin(t1)\n",
        "    f2 = (l1 / l2) * (w1 ** 2) * jnp.sin(t1 - t2) - (g / l2) * jnp.sin(t2)\n",
        "\n",
        "    g1 = (f1 - a1 * f2) / (1 - a1 * a2)\n",
        "    g2 = (f2 - a2 * f1) / (1 - a1 * a2)\n",
        "\n",
        "    return jnp.stack([w1, w2, g1, g2])  # returns column vector of time derivatives of theta 1 and 2\n",
        "\n",
        "\n",
        "def equation_of_motion(lagrangian, state, t=None):\n",
        "    q, q_t = jnp.split(state, 2)  # the initial configuration\n",
        "    q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n",
        "            @ (jax.grad(lagrangian, 0)(q, q_t)\n",
        "               - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n",
        "    return jnp.concatenate([q_t, q_tt]) # so what happens in essence is it takes in the intial arguments and using the E-L\n",
        "    # equations produces the time derivatives \n",
        "\n",
        "\n",
        "def solve_lagrangian(lagrangian, initial_state, **kwargs):\n",
        "    @partial(jax.jit, backend='cpu')\n",
        "    def f(initial_state):\n",
        "        return odeint(partial(equation_of_motion, lagrangian), initial_state, **kwargs)\n",
        "    return f(initial_state)\n",
        "\n",
        "@partial(jax.jit, backend='cpu')\n",
        "def solve_autograd(initial_state, times, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
        "    L = partial(lagrangian, m1=m1, m2=m2, l1=l1, l2=l2, g=g)\n",
        "    # Specifies the conditions of the system onto the lagrangian then solves for times\n",
        "    return solve_lagrangian(L, initial_state, t=times, rtol=1e-10, atol=1e-10)\n",
        "\n",
        "@partial(jax.jit, backend='cpu')\n",
        "def solve_analytical(initial_state, times):\n",
        "    # Just solves the exact analytical equations for different time steps\n",
        "    return odeint(f_analytical, initial_state, t=times, rtol=1e-10, atol=1e-10)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U64_M-X0btTh"
      },
      "source": [
        "# choose an initial state\n",
        "x0 = np.array([3*np.pi/7, 3*np.pi/4, 0, 0], dtype=np.float32)\n",
        "noise = np.random.RandomState(0).randn(x0.size)\n",
        "t = np.linspace(0, 40, num=401, dtype=np.float32) # evenly spaced time steps "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o05iDEewhjzb"
      },
      "source": [
        "# compute dynamics analytically\n",
        "x_analytical = solve_analytical(x0, t)\n",
        "noise = np.random.RandomState(0).randn(x0.size)\n",
        "noise_1, noise_2 = 1e-10, 1e-11\n",
        "x_perturbed_1 = solve_analytical(x0 + noise_1 * noise, t)\n",
        "x_perturbed_2 = solve_analytical(x0 + noise_2 * noise, t)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4Y_ZEZ0hg1r"
      },
      "source": [
        "# compute dynamics with the lagrangian \n",
        "x_autograd = jax.device_get(solve_autograd(x0, t))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOMX7cTVgnlJ"
      },
      "source": [
        "# generating training data \n",
        "\n",
        "time_step = 1\n",
        "N=1500\n",
        "# Use the x0 generated earlier \n",
        "t1=np.arange(N, dtype=np.float32) # t1 is an array evenly spaved from 0 to 1499\n",
        "t2= np.arange(N,2*N,dtype=np.float32)\n",
        "\n",
        "x_train = solve_analytical(x0,t1)\n",
        "xt_train = jax.vmap(f_analytical)(x_train) # finds the derivative of the inital state using the analytical method \n",
        "\n",
        "x_test = solve_analytical(x0,t2)\n",
        "xt_test = jax.vmap(f_analytical)(x_test)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXnQHuWXmhf8"
      },
      "source": [
        "# setting up to build the NN \n",
        "\n",
        "def normalize_dp(state):\n",
        "  # rescales the coordinates to the minus pi to pi range \n",
        "  return jnp.concatenate([(state[:2] + np.pi) % (2 * np.pi) - np.pi, state[2:]])\n",
        "\n",
        "def lagrangian_paramteric(params): # we replace the lagrangian we know with the black box lagrangian \n",
        "  def lagrangian(q,q_t):\n",
        "    assert q.shape == (2,)\n",
        "    state = normalise_dp(jnp.concatenate([q,q_t])) \n",
        "    return jnp.squeeze(nn_forward_fn(params, state), axis=-1) # look at the idea of np.squeeze online \n",
        "  return lagrangian\n",
        "\n",
        "# create the loss function for the problem \n",
        "\n",
        "@jax.jit\n",
        "def loss(params, batch, time_step=None):\n",
        "  state, targets = batch # the batch consists of state = x_train and targets = xt_train\n",
        "  if time_step is not None:\n",
        "    f = partial(equation_of_motion, learned_lagrangian(params))\n",
        "    preds = jax.vmap(partial(rk4_step, f, t=0.0, h=time_step))(state) # we integrate the time derivates from the E-L \n",
        "    # equation according to the black box lagranian and obtain the next step state \n",
        "  else:  # time_step is asserted = 0.001 and never changed so surely we always got through the if statement ? \n",
        "    preds = jax.vmap(partial(equation_of_motion, learned_lagrangian(params)))(state)\n",
        "  return jnp.mean((preds - targets) ** 2)\n",
        "\n",
        "\n",
        "# build the neural network model 5 layers starting with 128 nodes \n",
        "init_random_params, nn_forward_fn = stax.serial(\n",
        "    stax.Dense(128),\n",
        "    stax.Softplus,\n",
        "    stax.Dense(128),\n",
        "    stax.Softplus,\n",
        "    stax.Dense(1),\n",
        ")\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XduMIAkwrvD"
      },
      "source": [
        "@jax.jit\n",
        "def update_derivative(i, opt_state, batch):\n",
        "  params = get_params(opt_state)\n",
        "  return opt_update(i, jax.grad(loss)(params, batch, None), opt_state)\n",
        "\n",
        "# I omitted update_timestep as we don't use it \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "VPq4EBLcwx_u",
        "outputId": "8cfe5bc5-0345-4e1d-d337-0f16c2d13484"
      },
      "source": [
        "%%time\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "init_params = init_random_params(rng, (-1, 4)) # chose some random parameters \n",
        "\n",
        "batch_size = 100\n",
        "test_every = 10\n",
        "num_batches = 1500\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# adam w learn rate decay\n",
        "opt_init, opt_update, get_params = optimizers.adam(\n",
        "    lambda t: jnp.select([t < batch_size*(num_batches//3),\n",
        "                          t < batch_size*(2*num_batches//3),\n",
        "                          t > batch_size*(2*num_batches//3)],\n",
        "                         [1e-3, 3e-4, 1e-4])) # select just looks at the condition list and selects from the choices \n",
        "opt_state = opt_init(init_params)\n",
        "\n",
        "for iteration in range(batch_size*num_batches + 1):\n",
        "  if iteration % batch_size == 0:\n",
        "    params = get_params(opt_state)\n",
        "    train_loss = loss(params, (x_train, xt_train))\n",
        "    train_losses.append(train_loss)\n",
        "    test_loss = loss(params, (x_test, xt_test))\n",
        "    test_losses.append(test_loss)\n",
        "    if iteration % (batch_size*test_every) == 0:\n",
        "      print(f\"iteration={iteration}, train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\")\n",
        "  opt_state = update_derivative(iteration, opt_state, (x_train, xt_train))\n",
        "\n",
        "params = get_params(opt_state)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-708a7e91ec56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nrng = jax.random.PRNGKey(0)\\ninit_params = init_random_params(rng, (-1, 4)) # chose some random parameters \\n\\nbatch_size = 100\\ntest_every = 10\\nnum_batches = 1500\\n\\ntrain_losses = []\\ntest_losses = []\\n\\n# adam w learn rate decay\\nopt_init, opt_update, get_params = optimizers.adam(\\n    lambda t: jnp.select([t < batch_size*(num_batches//3),\\n                          t < batch_size*(2*num_batches//3),\\n                          t > batch_size*(2*num_batches//3)],\\n                         [1e-3, 3e-4, 1e-4])) #\\xa0select just looks at the condition list and selects from the choices \\nopt_state = opt_init(init_params)\\n\\nfor iteration in range(batch_size*num_batches + 1):\\n  if iteration % batch_size == 0:\\n    params = get_params(opt_state)\\n    train_loss = loss(params, (x_train, xt_train))\\n    train_losses.append(train_loss)\\n    test_loss = loss(params, (x_test, xt_test))\\n    test_losses.append(test_loss)\\n    if iteration % (batch_size*test_every) == 0:\\n      print(f\"iteration={iteration}, train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\")\\n  opt_state = update_derivative(iteration, opt_state, (x_train, xt_train))\\n\\nparams = get_params(opt_state)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'init_random_params' is not defined"
          ]
        }
      ]
    }
  ]
}